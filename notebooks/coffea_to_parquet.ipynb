{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf294b6d-44a6-4fa4-815c-9aa89c118b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import awkward as ak\n",
    "\n",
    "import vector\n",
    "vector.register_numba()\n",
    "vector.register_awkward()\n",
    "\n",
    "from coffea.util import load\n",
    "from coffea.processor.accumulator import column_accumulator\n",
    "from coffea.processor import accumulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebeb423-1541-48ad-8491-a1631c5075cc",
   "metadata": {},
   "source": [
    "# Loading the exported dataset\n",
    "\n",
    "We open the .coffea file and read the output accumulator. The ntuples for the training are saved under the key `columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c384868-2dbb-4410-bfe9-f9d95d5e2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = \"2017\"\n",
    "sample = \"ttHTobb_ttToSemiLep\"\n",
    "df = load(f\"/eos/user/m/mmarcheg/ttHbb/training_datasets/{year}/output_{sample}_{year}.coffea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb943f2-26dd-4deb-b88a-9f1a0646a4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial': {'ttHTobb_ttToSemiLep_2017': 9992000},\n",
       " 'skim': {'ttHTobb_ttToSemiLep_2017': 2572504},\n",
       " 'presel': {'ttHTobb_ttToSemiLep_2017': 1361657},\n",
       " 'baseline': {'ttHTobb_ttToSemiLep_2017': {'ttHTobb_ttToSemiLep': 1361657}},\n",
       " 'semilep_LHE': {'ttHTobb_ttToSemiLep_2017': {'ttHTobb_ttToSemiLep': 1277812}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cutflow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a10b95c-625b-49b2-9c98-ab0e8e9ef006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'ttHTobb_ttToSemiLep_2017': {'ttHTobb_ttToSemiLep': 646.6523174590126}},\n",
       " 'semilep_LHE': {'ttHTobb_ttToSemiLep_2017': {'ttHTobb_ttToSemiLep': 606.8207020523254}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sumw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5dbbc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ttHTobb_ttToSemiLep_2017': 2249755.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sum_genweights\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33eaa94-8475-474b-9566-929c0e5bd90a",
   "metadata": {},
   "source": [
    "## Normalize the genweights\n",
    "\n",
    "Since the array `weight` is filled on the fly with the weight associated with the event, it does not take into account the overall scaling by the sum of genweights (`sum_genweights`).\n",
    "In order to correct for this, we have to scale by hand the `weight` array dividing by the sum of genweights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b7b9a0-a881-40a3-a307-d4cb52fa6c39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:  dict_keys(['ttHTobb_ttToSemiLep_2017'])\n"
     ]
    }
   ],
   "source": [
    "datasets = df[\"sum_genweights\"].keys()\n",
    "print(\"Datasets: \", datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b08d594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "column_accumulator(array([1500.0080289 , 1089.91516367, 1150.18742552, ..., 1094.61906636,\n",
       "        902.0117143 , 1089.56201334]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = df[\"columns\"][sample][f\"{sample}_{year}\"][\"semilep_LHE\"][\"weight\"]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4994875e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['weight', 'Parton_N', 'Parton_pt', 'Parton_eta', 'Parton_phi', 'Parton_mass', 'Parton_pdgId', 'Parton_provenance', 'PartonMatched_N', 'PartonMatched_pt', 'PartonMatched_eta', 'PartonMatched_phi', 'PartonMatched_mass', 'PartonMatched_pdgId', 'PartonMatched_provenance', 'PartonMatched_dRMatchedJet', 'JetGood_N', 'JetGood_pt', 'JetGood_eta', 'JetGood_phi', 'JetGood_hadronFlavour', 'JetGood_btagDeepFlavB', 'JetGoodMatched_N', 'JetGoodMatched_pt', 'JetGoodMatched_eta', 'JetGoodMatched_phi', 'JetGoodMatched_hadronFlavour', 'JetGoodMatched_btagDeepFlavB', 'JetGoodMatched_dRMatchedJet', 'LeptonGood_pt', 'LeptonGood_eta', 'LeptonGood_phi', 'LeptonGood_pdgId', 'LeptonGood_charge', 'LeptonGood_mvaTTH', 'MET_phi', 'MET_pt', 'MET_significance', 'Generator_x1', 'Generator_x2', 'Generator_id1', 'Generator_id2', 'Generator_xpdf1', 'Generator_xpdf2', 'LeptonParton_N', 'LeptonParton_pt', 'LeptonParton_eta', 'LeptonParton_phi', 'LeptonParton_mass', 'LeptonParton_pdgId', 'HiggsParton_pt', 'HiggsParton_eta', 'HiggsParton_phi', 'HiggsParton_mass', 'HiggsParton_pdgId'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"columns\"][sample][f\"{sample}_{year}\"][\"semilep_LHE\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6105294e-6066-4345-b080-87c56b85fb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    df[\"columns\"][sample][dataset][\"semilep_LHE\"][\"weight\"] = column_accumulator(df[\"columns\"][sample][dataset][\"semilep_LHE\"][\"weight\"].value / df[\"sum_genweights\"][dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68a2d825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "column_accumulator(array([0.00066674, 0.00048446, 0.00051125, ..., 0.00048655, 0.00040094,\n",
       "       0.0004843 ]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_new = df[\"columns\"][sample][f\"{sample}_{year}\"][\"semilep_LHE\"][\"weight\"]\n",
    "w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1fbb14",
   "metadata": {},
   "source": [
    "## Accumulate ntuples from different data-taking eras\n",
    "\n",
    "In order to enlarge our training sample, we merge ntuples coming from different data-taking eras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4810b3c2-60bf-456b-8b92-acc1d306bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = accumulate([df[\"columns\"][sample][dataset][\"semilep_LHE\"] for dataset in datasets])\n",
    "\n",
    "partons = ak.unflatten(ak.zip({\"pt\": cs[\"Parton_pt\"].value,\n",
    "                              \"eta\": cs[\"Parton_eta\"].value,\n",
    "                              \"phi\": cs[\"Parton_phi\"].value,\n",
    "                               \"mass\": cs[\"Parton_mass\"].value,\n",
    "                              \"pdgId\": cs[\"Parton_pdgId\"].value, \n",
    "                              \"prov\": cs[\"Parton_provenance\"].value},\n",
    "                         with_name='Momentum4D'),\n",
    "                     cs[\"Parton_N\"].value)\n",
    "\n",
    "partons_matched = ak.unflatten(ak.zip({\"pt\": cs[\"PartonMatched_pt\"].value,\n",
    "                              \"eta\": cs[\"PartonMatched_eta\"].value,\n",
    "                              \"phi\": cs[\"PartonMatched_phi\"].value,\n",
    "                              \"mass\": cs[\"PartonMatched_mass\"].value,\n",
    "                              \"pdgId\": cs[\"PartonMatched_pdgId\"].value, \n",
    "                              \"prov\": cs[\"PartonMatched_provenance\"].value},\n",
    "                         with_name='Momentum4D'),\n",
    "                     cs[\"PartonMatched_N\"].value)\n",
    "\n",
    "jets = ak.unflatten(ak.zip({\"pt\": cs[\"JetGood_pt\"].value,\n",
    "                              \"eta\": cs[\"JetGood_eta\"].value,\n",
    "                              \"phi\": cs[\"JetGood_phi\"].value,\n",
    "                              \"btag\": cs[\"JetGood_btagDeepFlavB\"].value,\n",
    "                              \"m\": np.zeros_like(cs[\"JetGood_pt\"].value)},\n",
    "                         with_name='Momentum4D'),\n",
    "                     cs[\"JetGood_N\"].value)\n",
    "\n",
    "jets_matched = ak.unflatten(ak.zip({\"pt\": cs[\"JetGoodMatched_pt\"].value,\n",
    "                              \"eta\": cs[\"JetGoodMatched_eta\"].value,\n",
    "                              \"phi\": cs[\"JetGoodMatched_phi\"].value,\n",
    "                              \"btag\": cs[\"JetGoodMatched_btagDeepFlavB\"].value,\n",
    "                              \"prov\": cs[\"PartonMatched_provenance\"].value,\n",
    "                              \"m\": np.zeros_like(cs[\"JetGoodMatched_pt\"].value)},\n",
    "                         with_name='Momentum4D'),\n",
    "                     cs[\"JetGoodMatched_N\"].value)\n",
    "\n",
    "\n",
    "generator_info = ak.zip({\"pdgid1\": cs[\"Generator_id1\"].value,\n",
    "                              \"pdgid2\": cs[\"Generator_id2\"].value,\n",
    "                              \"x1\": cs[\"Generator_x1\"].value,\n",
    "                              \"x2\": cs[\"Generator_x2\"].value},\n",
    "                         )\n",
    "\n",
    "\n",
    "lepton_partons = ak.unflatten(ak.zip({\"pt\": cs[\"LeptonParton_pt\"].value,\n",
    "                              \"eta\": cs[\"LeptonParton_eta\"].value,\n",
    "                              \"phi\": cs[\"LeptonParton_phi\"].value,\n",
    "                              \"mass\": cs[\"LeptonParton_mass\"].value,\n",
    "                              \"pdgId\": cs[\"LeptonParton_pdgId\"].value},\n",
    "                         with_name='Momentum4D'),\n",
    "                     cs[\"LeptonParton_N\"].value)\n",
    "\n",
    "\n",
    "lepton = ak.zip({\"pt\": cs[\"LeptonGood_pt\"].value,\n",
    "                              \"eta\": cs[\"LeptonGood_eta\"].value,\n",
    "                              \"phi\": cs[\"LeptonGood_phi\"].value,\n",
    "                              \"pdgId\": cs[\"LeptonGood_pdgId\"].value,\n",
    "                              \"m\": np.zeros_like(cs[\"LeptonGood_pt\"].value)},\n",
    "                         with_name='Momentum4D')\n",
    "\n",
    "\n",
    "met = ak.zip({\"pt\": cs[\"MET_pt\"].value,\n",
    "              \"eta\":  np.zeros_like(cs[\"MET_pt\"].value),\n",
    "              \"phi\": cs[\"MET_phi\"].value,\n",
    "              \"m\": np.zeros_like(cs[\"MET_pt\"].value)},\n",
    "         with_name='Momentum4D')\n",
    "\n",
    "higgs = ak.zip({\"pt\": cs[\"HiggsParton_pt\"].value,\n",
    "                              \"eta\": cs[\"HiggsParton_eta\"].value,\n",
    "                              \"phi\": cs[\"HiggsParton_phi\"].value,\n",
    "                              \"m\": cs[\"HiggsParton_mass\"].value},\n",
    "                         with_name='Momentum4D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef24202e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14270152939384279"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak.sum(jets_matched.pt == -999) / ak.count(jets_matched.pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51924b95-be03-4afa-bd6d-9355d8504c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jets_matched = ak.mask(jets_matched, jets_matched.pt==-999, None)\n",
    "partons_matched = ak.mask(partons_matched, partons_matched.pt==-999, None)\n",
    "is_jet_matched = ~ak.is_none(jets_matched, axis=1)\n",
    "jets = ak.with_field(jets, is_jet_matched, \"matched\")\n",
    "\n",
    "# Filling with -1 the not matched provenance\n",
    "jets = ak.with_field(jets, ak.fill_none(jets_matched.prov, -1), \"prov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2e3c8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [[True, True, True, ... True, True]] type='1277812 * var * bool'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jets.matched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d88c0-b9dd-4849-933d-12997bfd1be4",
   "metadata": {},
   "source": [
    "Jets and partons_matched arrays are **aligned**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06eb64b-e5cd-4ce4-ae47-7b5531e0d7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = ak.zip({\n",
    "    \"jets\": jets,\n",
    "    \"partons_matched\": partons_matched,\n",
    "    \"partons\": partons,\n",
    "    \"generator_info\": generator_info,\n",
    "    \"lepton_partons\":lepton_partons,\n",
    "    \"lepton_reco\": lepton,\n",
    "    \"met\": met,\n",
    "    \"higgs\": higgs\n",
    "    }, depth_limit=1)\n",
    "\n",
    "ak.to_parquet(dfout, \"/eos/user/m/mmarcheg/ttHbb/training_datasets/test/ttHTobb_ttToSemiLep_2017_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d762c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
